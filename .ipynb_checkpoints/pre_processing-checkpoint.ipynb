{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e7e80e-0a96-4fe4-8230-c85dfddad43f",
   "metadata": {},
   "source": [
    "# Data Exploration and Data Pre-processing\n",
    "\n",
    "This Notebook sets up and configures a Spark session that involves processing large datasets from Swiss public transportation systems (SBB). The code includes detailed data cleaning, filtering, and transformation steps to prepare the data for further analysis.The goal is to prepare the data for robust journey planning applications by filtering out irrelevant records, managing data types, and ensuring data integrity and accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cdaf75-538f-45e1-8443-d2b383e0e9c9",
   "metadata": {},
   "source": [
    "# Initialize the Spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f41897f2-9ac1-4793-8780-3d151e97de1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorMemory': '4G', 'executorCores': 4, 'numExecutors': 10, 'conf': {'spark.executorEnv.USERNAME': 'malahlou', 'spark.executorEnv.HADOOP_DEFAULT_FS': 'hdfs://iccluster067.iccluster.epfl.ch:8020', 'mapreduce.input.fileinputformat.input.dir.recursive': True, 'spark.sql.extensions': 'com.hortonworks.spark.sql.rule.Extensions', 'spark.kryo.registrator': 'com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator', 'spark.sql.hive.hiveserver2.jdbc.url': 'jdbc:hive2://iccluster065.iccluster.epfl.ch:2181,iccluster080.iccluster.epfl.ch:2181,iccluster066.iccluster.epfl.ch:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2', 'spark.datasource.hive.warehouse.read.mode': 'JDBC_CLUSTER', 'spark.driver.extraClassPath': '/opt/cloudera/parcels/SPARK3/lib/hwc_for_spark3/hive-warehouse-connector-spark3-assembly-1.0.0.3.3.7190.2-1.jar', 'spark.executor.extraClassPath': '/opt/cloudera/parcels/SPARK3/lib/hwc_for_spark3/hive-warehouse-connector-spark3-assembly-1.0.0.3.3.7190.2-1.jar', 'spark.kryoserializer.buffer.max': '2000m'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>5770</td><td>application_1713270977862_6554</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster080.iccluster.epfl.ch:8088/proxy/application_1713270977862_6554/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1713270977862_6554_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{ \"conf\": {\n",
    "        \"mapreduce.input.fileinputformat.input.dir.recursive\": true,\n",
    "        \"spark.sql.extensions\": \"com.hortonworks.spark.sql.rule.Extensions\",\n",
    "        \"spark.kryo.registrator\": \"com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator\",\n",
    "        \"spark.sql.hive.hiveserver2.jdbc.url\": \"jdbc:hive2://iccluster065.iccluster.epfl.ch:2181,iccluster080.iccluster.epfl.ch:2181,iccluster066.iccluster.epfl.ch:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2\",\n",
    "        \"spark.datasource.hive.warehouse.read.mode\": \"JDBC_CLUSTER\",\n",
    "        \"spark.driver.extraClassPath\": \"/opt/cloudera/parcels/SPARK3/lib/hwc_for_spark3/hive-warehouse-connector-spark3-assembly-1.0.0.3.3.7190.2-1.jar\",\n",
    "        \"spark.executor.extraClassPath\": \"/opt/cloudera/parcels/SPARK3/lib/hwc_for_spark3/hive-warehouse-connector-spark3-assembly-1.0.0.3.3.7190.2-1.jar\",\n",
    "        \"spark.kryoserializer.buffer.max\": \"2000m\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48c5e82-d78b-4208-b2d0-90f7b9e1db02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>5772</td><td>application_1713270977862_6556</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster080.iccluster.epfl.ch:8088/proxy/application_1713270977862_6556/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1713270977862_6556_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Spark name:livy-session-5772, version:3.3.2.3.3.7190.2-1"
     ]
    }
   ],
   "source": [
    "print(f'Start Spark name:{spark._sc.appName}, version:{spark.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c0ab26-2705-483e-b312-0a5de3a305ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local username=malahlou\n",
      "hadoop_fs=hdfs://iccluster067.iccluster.epfl.ch:8020\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "import os\n",
    "username=os.getenv('USER', 'anonymous')\n",
    "hadoop_fs=os.getenv('HADOOP_DEFAULT_FS', 'hdfs://iccluster067.iccluster.epfl.ch:8020')\n",
    "print(f\"local username={username}\\nhadoop_fs={hadoop_fs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9379bc7-a06a-4495-a3f9-cbb8e271f434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote username=malahlou\n",
      "hadoop_fs=hdfs://iccluster067.iccluster.epfl.ch:8020"
     ]
    }
   ],
   "source": [
    " # (prevent deprecated np.bool error since numpy 1.24, until a new version of pandas/Spark fixes this)\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "\n",
    "username=spark.conf.get('spark.executorEnv.USERNAME', 'anonymous')\n",
    "hadoop_fs=spark.conf.get('spark.executorEnv.HADOOP_DEFAULT_FS','hdfs://iccluster067.iccluster.epfl.ch:8020')\n",
    "print(f\"remote username={username}\\nhadoop_fs={hadoop_fs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe52b6d2-8c1c-4c69-a70f-fa76ff8560e6",
   "metadata": {},
   "source": [
    "# Start the pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaedae0-6b50-4460-b888-9c616f6d3713",
   "metadata": {},
   "source": [
    "From the exercises,we know that the stops data contains the information about the stops geographical locations. The stops are provided with the SBB timetables and are updated once a week (usually on Wednesdays).\n",
    "Since we know that it is updated usually on wednesdays, we picked February 21st."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea8eae20-041a-4add-a2aa-5b65cb072a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_path = '/data/sbb/orc/timetables/'\n",
    "date = 'year=2024/month=2/day=21/'\n",
    "\n",
    "def generate_path(base_path, dataset, date):\n",
    "    return f\"{base_path}{dataset}/{date}\"\n",
    "\n",
    "stops_path = generate_path(base_path, 'stops', date)\n",
    "stop_times_path = generate_path(base_path, 'stop_times', date)\n",
    "trips_path = generate_path(base_path, 'trips', date)\n",
    "calendar_path = generate_path(base_path, 'calendar', date)\n",
    "\n",
    "stops = spark.read.orc(stops_path)\n",
    "stop_times = spark.read.orc(stop_times_path)\n",
    "trips = spark.read.orc(trips_path)\n",
    "calendar = spark.read.orc(calendar_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451b5e6-08a3-43fa-8760-90d83a22b04c",
   "metadata": {},
   "source": [
    "#### Keep only services that operate on weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52069973-a9bc-4206-a811-a88ea3247813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "weekdays = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday']\n",
    "condition = reduce(lambda acc, day: acc & (col(day) == 'TRUE'), weekdays[1:], col(weekdays[0]) == 'TRUE')\n",
    "weekday_ids = calendar.filter(condition).select('service_id')\n",
    "weekday_trips = trips.join(weekday_ids, on='service_id', how='inner').distinct()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd241d-99e8-40d2-b1f0-21d96fe38866",
   "metadata": {},
   "source": [
    "Merge with the stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57aaba47-5859-4486-a3fe-a1095f4ff484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nodes = weekday_trips.join(stop_times, on='trip_id', how='inner')\n",
    "final_nodes = stops.join(nodes, on='stop_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01937065-e064-4539-92be-f3ee3e3eab6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "final_nodes_arr = (final_nodes.drop(\"departure_time\")\n",
    "             .withColumnRenamed(\"arrival_time\",\"time\")\n",
    "             .withColumn(\"is_arrival\",lit(1)))\n",
    "\n",
    "final_nodes_dep = (final_nodes.drop(\"arrival_time\")\n",
    "             .withColumnRenamed(\"departure_time\",\"time\")\n",
    "             .withColumn(\"is_arrival\", lit(0)))\n",
    "\n",
    "final_nodes = final_nodes_arr.union(final_nodes_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359db0cf-663d-4a39-9a2e-181a351dfa84",
   "metadata": {},
   "source": [
    "### Enable support for ESRI UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5122537-fcb8-4a06-864c-b5fdd0e8829e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "ADD JARS\n",
    "    {hadoop_fs}/data/jars/esri-geometry-api-2.2.4.jar\n",
    "    {hadoop_fs}/data/jars/spatial-sdk-hive-2.2.0.jar\n",
    "    {hadoop_fs}/data/jars/spatial-sdk-json-2.2.0.jar\n",
    "\"\"\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY FUNCTION ST_Point AS 'com.esri.hadoop.hive.ST_Point'\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY FUNCTION ST_Distance AS 'com.esri.hadoop.hive.ST_Distance'\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY FUNCTION ST_SetSRID AS 'com.esri.hadoop.hive.ST_SetSRID'\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY FUNCTION ST_GeodesicLengthWGS84 AS 'com.esri.hadoop.hive.ST_GeodesicLengthWGS84'\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY FUNCTION ST_LineString AS 'com.esri.hadoop.hive.ST_LineString'\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY FUNCTION ST_AsBinary AS 'com.esri.hadoop.hive.ST_AsBinary'\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY FUNCTION ST_PointFromWKB AS 'com.esri.hadoop.hive.ST_PointFromWKB'\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY FUNCTION ST_GeomFromWKB AS 'com.esri.hadoop.hive.ST_GeomFromWKB'\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY FUNCTION ST_Contains AS 'com.esri.hadoop.hive.ST_Contains'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960f3ba-6bbf-4998-8dc9-e23576ad4673",
   "metadata": {},
   "source": [
    "### Choose the region : Lausanne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719bec9-bb8a-46d0-9ca7-4fcf6681f10f",
   "metadata": {},
   "source": [
    "Find stops within a region, here we choose Lausanne region, but to change the Lausanne region, you need to change the stop objectid, equals right now to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6feb88fd-feda-44bb-8532-04ad39671a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed25d9d2-8398-4318-8b24-f21658ece633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_nodes.createOrReplaceTempView(\"final_nodes\")\n",
    "geo_shapes = spark.read.table(\"com490.geo_shapes\")\n",
    "geo_shapes.createOrReplaceTempView(\"geo_shapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab61cd50-5701-47bb-8a98-b1693c1c971f",
   "metadata": {},
   "source": [
    "9801210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "525f1b4e-1163-4b6c-9749-193e657503a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stops_area = spark.sql(f\"\"\"\n",
    "SELECT\n",
    "    n.stop_id,\n",
    "    n.stop_name,\n",
    "    n.stop_lat,\n",
    "    n.stop_lon,\n",
    "    n.location_type,\n",
    "    n.parent_station,\n",
    "    n.trip_id,\n",
    "    n.service_id,\n",
    "    n.route_id,\n",
    "    n.trip_headsign,\n",
    "    n.trip_short_name,\n",
    "    n.direction_id,\n",
    "    n.time,\n",
    "    n.stop_sequence,\n",
    "    n.pickup_type,\n",
    "    n.drop_off_type,\n",
    "    n.is_arrival\n",
    "FROM final_nodes n\n",
    "JOIN com490.geo_shapes g\n",
    "ON ST_Contains(g.geometry, ST_Point(n.stop_lon, n.stop_lat))\n",
    "WHERE g.objectid = {object_id}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ca71953-c94c-4b1a-b016-41816fef680d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nodes_area = stops_area.select(stops_area.stop_id,\n",
    "                               stops_area.stop_name,\n",
    "                               stops_area.stop_lat,\n",
    "                               stops_area.stop_lon,\n",
    "                               stops_area.parent_station).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af8eee-a917-4785-ada2-78ce17502382",
   "metadata": {},
   "source": [
    "We choose unique ids such that it gives information about the stop id, the time, the trip id and if the nodes corresponds to an arrival or a departure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09bb6142-075f-455d-974e-e1d23a403918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "stops_area = stops_area.withColumn(\"unique_stop_id\",F.concat_ws(\"_\", stops_area.stop_id,stops_area.time,\n",
    "                                                stops_area.trip_id,stops_area.is_arrival))\n",
    "filtered_data = stops_area.select(\"stop_id\", \"trip_id\",\"route_id\",\"unique_stop_id\",\"time\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616c8a2-d969-4c58-806c-d45a80311ad1",
   "metadata": {},
   "source": [
    "### Building edges\n",
    "We add edges to connect arrival nodes and departure nodes within the same station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f619e15-fdba-4fff-b6c1-c6e58d1650de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select relevant columns for arrival and departure\n",
    "arrival_data = filtered_data.filter(F.col(\"is_arrival\") == 1).select(\n",
    "    F.col(\"stop_id\"),\n",
    "    F.col(\"trip_id\"),\n",
    "    F.col(\"route_id\"),\n",
    "    F.col(\"unique_stop_id\"),\n",
    "    F.col(\"time\").alias(\"arrival_time\")\n",
    ")\n",
    "\n",
    "\n",
    "departure_data = filtered_data.filter(F.col(\"is_arrival\") == 0).select(\n",
    "    F.col(\"stop_id\"),\n",
    "    F.col(\"trip_id\"),\n",
    "    F.col(\"route_id\"),\n",
    "    F.col(\"unique_stop_id\"),\n",
    "    F.col(\"time\").alias(\"departure_time\")\n",
    ")\n",
    "\n",
    "\n",
    "# Join stop_times with filtered arrival and departure data\n",
    "filtered_data_arr = stop_times.join(arrival_data, on=[\"stop_id\", \"trip_id\", \"arrival_time\"], how=\"inner\")\n",
    "filtered_data_dep = stop_times.join(departure_data, on=[\"stop_id\", \"trip_id\", \"departure_time\"], how=\"inner\")\n",
    "\n",
    "# Union the results\n",
    "filtered_data = filtered_data_dep.union(filtered_data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe25574e-31bd-4db2-81dd-2207425072f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "window_spec = Window.partitionBy('trip_id').orderBy([\n",
    "    col('stop_sequence').asc(),\n",
    "    col('departure_time').asc(),\n",
    "    col('unique_stop_id').desc()\n",
    "])\n",
    "\n",
    "filtered_data_pairs = filtered_data.withColumn('stop_id_dest', F.lead('stop_id').over(window_spec)) \\\n",
    "    .withColumn('arrival_time_dest', F.lead('arrival_time').over(window_spec)) \\\n",
    "    .withColumn('unique_stop_id_dest', F.lead('unique_stop_id').over(window_spec))\n",
    "\n",
    "filtered_data_pairs = filtered_data_pairs.drop('arrival_time') \\\n",
    "    .withColumnRenamed('arrival_time_dest', 'arrival_time')\n",
    "\n",
    "filtered_data_pairs = filtered_data_pairs.dropna(subset=['stop_id_dest'])\n",
    "\n",
    "filtered_data_pairs = filtered_data_pairs.withColumn(\n",
    "    'expected_travel_time',\n",
    "    F.unix_timestamp('arrival_time', 'HH:mm:ss') - F.unix_timestamp('departure_time', 'HH:mm:ss')\n",
    ")\n",
    "\n",
    "filtered_data_pairs = filtered_data_pairs.filter(col('stop_id') != col('stop_id_dest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d6cb4c0-1ae0-488c-84c4-b54e0f825365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split, lit\n",
    "\n",
    "filtered_data_pairs = filtered_data_pairs.select(\n",
    "    col(\"unique_stop_id\").alias(\"start_id\"),\n",
    "    col(\"unique_stop_id_dest\").alias(\"end_id\"),\n",
    "    col(\"expected_travel_time\")\n",
    ")\n",
    "\n",
    "# Split start_id column into separate columns\n",
    "filtered_data_pairs = filtered_data_pairs.withColumn(\"start_stop_id\", split(col(\"start_id\"), \"_\")[0]) \\\n",
    "    .withColumn(\"start_time\", split(col(\"start_id\"), \"_\")[1]) \\\n",
    "    .withColumn(\"trip_id\", split(col(\"start_id\"), \"_\")[2])\n",
    "\n",
    "# Split end_id column into separate columns\n",
    "filtered_data_pairs = filtered_data_pairs.withColumn(\"end_stop_id\", split(col(\"end_id\"), \"_\")[0]) \\\n",
    "    .withColumn(\"end_time\", split(col(\"end_id\"), \"_\")[1])\n",
    "\n",
    "filtered_data_pairs = filtered_data_pairs.drop(\"start_id\", \"end_id\")\n",
    "\n",
    "filtered_data_pairs = filtered_data_pairs.withColumn(\"is_walking\", lit(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e9b2f3-fdcb-4d8b-819d-eeb91fa49915",
   "metadata": {},
   "source": [
    "### Walking edges\n",
    "We add edges between close stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91689288-7437-4e50-b4b2-be9acaaf05c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "time_in_station = 2 * 60\n",
    "\n",
    "# Filter stops with non-null parent_station\n",
    "filtered_stops = stops_area.filter(col(\"parent_station\").isNotNull())\n",
    "\n",
    "# Self-join to find possible transfers\n",
    "station_transfer_edges = (\n",
    "    filtered_stops.alias(\"a\")\n",
    "    .join(\n",
    "        filtered_stops.alias(\"b\"),\n",
    "        (col(\"a.parent_station\") == col(\"b.parent_station\")) &\n",
    "        (col(\"a.stop_id\") != col(\"b.stop_id\")),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"a.stop_id\").alias(\"stop_1\"),\n",
    "        col(\"b.stop_id\").alias(\"stop_2\")\n",
    "    )\n",
    "    .withColumn(\"transfer_time\", lit(time_in_station))\n",
    "    .distinct()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2da63179-e84c-4d8c-8ffe-6ccc65703837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, round\n",
    "\n",
    "max_walking_distance = 500  # meters\n",
    "walking_speed = 50.0  # meters per minute\n",
    "\n",
    "# Filter and prepare stops data\n",
    "filtered_stops = stops_area.select(\n",
    "    col(\"stop_id\").alias(\"stop_1\"),\n",
    "    col(\"stop_lat\").alias(\"lat_1\"),\n",
    "    col(\"stop_lon\").alias(\"lon_1\"),\n",
    "    col(\"parent_station\").alias(\"par_1\")\n",
    ").distinct()\n",
    "\n",
    "# Self-join to find pairs of stops within walking distance\n",
    "walking_distance_station_edges = (\n",
    "    filtered_stops.alias(\"a\")\n",
    "    .crossJoin(\n",
    "        filtered_stops.alias(\"b\")\n",
    "    )\n",
    "    .filter(expr(\"a.stop_1 != b.stop_1\"))\n",
    "    .withColumn(\"line\", expr(\"ST_SetSRID(ST_LineString(a.lon_1, a.lat_1, b.lon_1, b.lat_1), 4326)\"))\n",
    "    .withColumn(\"distance\", expr(\"ST_GeodesicLengthWGS84(line)\"))\n",
    "    .filter((col(\"distance\") <= max_walking_distance) & (col(\"distance\") > 0.0))\n",
    "    .withColumn(\"transfer_time\", round(col(\"distance\") * (60.0 / walking_speed), 0))\n",
    "    .select(\n",
    "        col(\"a.stop_1\"),\n",
    "        col(\"b.stop_1\").alias(\"stop_2\"),\n",
    "        \"transfer_time\"\n",
    "    )\n",
    "    .distinct()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91622de0-ef9d-46e6-aaf3-ecf0e30d92f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#station_transfer_edges.show(2, vertical=True, truncate=True)\n",
    "#station_transfer_edges.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28e18892-809a-4266-9908-4b0228f488ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#walking_distance_station_edges.show(2, vertical=True, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff30a4-b95b-4fd5-8f10-70fb4fc3616b",
   "metadata": {},
   "source": [
    "3474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71b3a46c-cac3-4835-8189-d1ed4179c923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "walking_edges = (walking_distance_station_edges.union(station_transfer_edges)).withColumn(\"is_walking\",lit(1))#\n",
    "walking_edges = walking_edges.withColumnRenamed(\"stop_1\",\"start_stop_id\").withColumnRenamed(\"stop_2\",\"end_stop_id\").withColumnRenamed(\"transfer_time\",\"expected_travel_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00ffb8e8-41c6-4eb0-b749-5d3acb67198d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "walking_edges_copy = walking_edges.withColumn(\"trip_id\", lit(None)).withColumn(\"start_time\", lit(None)).withColumn(\"end_time\", lit(None))\n",
    "all_edges = filtered_data_pairs.unionByName(walking_edges_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "890c3439-4813-4319-8570-b6eaf29ed900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[expected_travel_time: double, start_stop_id: string, start_time: string, trip_id: string, end_stop_id: string, end_time: string, is_walking: int]"
     ]
    }
   ],
   "source": [
    "all_edges.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d193db2-7871-48d6-b274-dbcf7489f363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[stop_id: string, stop_name: string, stop_lat: decimal(15,12), stop_lon: decimal(15,12), parent_station: string]"
     ]
    }
   ],
   "source": [
    "nodes_area.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5411c073-34a0-4ec0-847f-777e2cc4dc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nodes_area_path = f\"/user/{username}/graph/nodes_area\"\n",
    "nodes_area.write.mode(\"overwrite\").format(\"orc\").save(nodes_area_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2972dd67-8794-453d-af27-84cf19bc3c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "py4j does not exist in the JVM\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 968, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 192, in deco\n",
      "    converted = convert_exception(e.java_exception)\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 156, in convert_exception\n",
      "    elif is_instance_of(gw, e, \"org.apache.spark.sql.AnalysisException\"):\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 464, in is_instance_of\n",
      "    return gateway.jvm.py4j.reflection.TypeUtil.isInstanceOf(\n",
      "  File \"/opt/cloudera/parcels/SPARK3-3.3.2.3.3.7190.2-1-1.p0.46867244/lib/spark3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1722, in __getattr__\n",
      "    raise Py4JError(message)\n",
      "py4j.protocol.Py4JError: py4j does not exist in the JVM\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_edges_path = f\"/user/{username}/graph/all_edges\"\n",
    "all_edges.write.mode(\"overwrite\").format(\"orc\").save(all_edges_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d64ee3c-d2b0-4b0c-8d00-e710b1d00aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921719ec-0570-44bd-926c-14ea16cfdd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
